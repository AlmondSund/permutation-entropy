{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; margin-top: 2em; margin-bottom: 2em;\">\n",
        "  <h1 style=\"font-size: 2.6em; margin-bottom: 0.4em;\">Entropía de Shannon</h1>\n",
        "  <p style=\"font-size: 1.1em;\">Un marco matemático riguroso para la medida de información</p>\n",
        "  <p style=\"font-size: 0.95em;\"><strong>Autor:</strong> Martín Ramírez Espinosa</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tabla de contenidos**\n",
        "\n",
        "- [1. Introducción](#introduccion)\n",
        "- [2. Preliminares y notación](#preliminares)\n",
        "- [3. Auto-información y entropía de Shannon](#definicion)\n",
        "- [4. Interpretaciones estadísticas y geométricas](#interpretaciones)\n",
        "- [5. Propiedades fundamentales](#propiedades)\n",
        "- [6. Caracterización axiomática](#axiomas)\n",
        "- [7. Ejemplos ilustrativos](#ejemplos)\n",
        "- [8. Entropía conjunta, condicional e información mutua](#extensiones)\n",
        "- [9. Desigualdades y límites](#desigualdades)\n",
        "- [10. Aplicaciones en codificación y ciencia de datos](#aplicaciones)\n",
        "- [11. Conclusiones y lecturas recomendadas](#conclusiones)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698132a4",
      "metadata": {},
      "source": [
        "### 1. Introducción <a id=\"introduccion\"></a>\n",
        "\n",
        "La noción de **entropía de Shannon** cuantifica la incertidumbre inherente a una variable aleatoria discreta. Desarrollada dentro de la teoría matemática de la comunicación de Claude E. Shannon (1948), esta medida constituye el cimiento formal que permite:\n",
        "\n",
        "1. Evaluar la cantidad mínima de bits necesaria para codificar información con pérdidas nulas.\n",
        "2. Analizar la eficiencia de códigos de compresión y de esquemas de transmisión.\n",
        "3. Relacionar fenómenos de probabilidad con nociones de sorpresa, variedad y desorden.\n",
        "\n",
        "La entropía traduce un sistema probabilístico en una cantidad escalar que captura la complejidad de describir sus posibles resultados. A diferencia de nociones intuitivas de desorden, la entropía de Shannon obedece axiomas precisos, satisface propiedades funcionales robustas y admite interpretaciones geométricas, termodinámicas y estadísticas. En este documento se desarrollan estas ideas con rigor matemático y se proveen ejemplos concretos que facilitan su comprensión.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628fb259",
      "metadata": {},
      "source": [
        "### 2. Preliminares y notación <a id=\"preliminares\"></a>\n",
        "\n",
        "Consideramos una variable aleatoria discreta $X$ definida sobre un espacio de probabilidad $(\\Omega, \\mathcal{F}, \\mathbb{P})$ con **alfabeto finito** $\\mathcal{X} = \\{x_1, \\dots, x_n\\}$. Denotamos por $p_i = \\mathbb{P}(X = x_i)$ la probabilidad del evento $X = x_i$. La distribución de probabilidad se representa mediante el vector $p = (p_1, \\dots, p_n)$ que satisface\n",
        "$$\n",
        "p_i \\geq 0 \\quad \\forall i \\in \\{1, \\dots, n\\}, \\qquad \\sum_{i=1}^n p_i = 1.\n",
        "$$\n",
        "\n",
        "Para la notación logarítmica utilizaremos la convención siguiente:\n",
        "\n",
        "- $\\log_b$ denota el logaritmo en base $b > 0$, $b \n",
        "eq 1$.\n",
        "- En teoría de la información resulta habitual emplear $\\log_2$ (bits) o $\\log_e$ (nats). Cuando la base no se especifica explícitamente, asumimos $\\log_2$.\n",
        "\n",
        "Se extiende la función $x \\mapsto x \\log x$ por continuidad definiendo $0 \\log 0 = 0$, puesto que $\\lim_{x \\downarrow 0} x \\log x = 0$.\n",
        "\n",
        "> **Definición 2.1 (Auto-información).** Para un resultado $x_i$ con probabilidad $p_i > 0$, la auto-información o sorpresa se define como\n",
        "> $$\n",
        "> I(x_i) = - \\log_b p_i.\n",
        "> $$\n",
        "> Esta magnitud expresa la cantidad de información aportada por la observación de $x_i$; valores menos probables producen mayor sorpresa.\n",
        "La entropía de Shannon emerge como el valor esperado de la auto-información, preservando así la estructura probabilística del modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5354f39",
      "metadata": {},
      "source": [
        "### 3. Auto-información y entropía de Shannon <a id=\"definicion\"></a>\n",
        "\n",
        "> **Definición 3.1 (Entropía de Shannon).** Sea $X$ una variable aleatoria discreta con distribución $p$. La entropía de Shannon de $X$ en base $b$ se define como\n",
        "> $$\n",
        "> H_b(X) = - \\sum_{i=1}^n p_i \\log_b p_i.\n",
        "> $$\n",
        "> Cuando la base es $2$ se escribe simplemente $H(X)$ y la unidad es el *bit*.\n",
        "Algunas observaciones esenciales:\n",
        "- La entropía es finita siempre que $p$ tenga soporte finito; si el alfabeto es numerable, la suma se interpreta como serie absolutamente convergente.\n",
        "- Se interpreta como el promedio de la sorpresa: $H_b(X) = \\mathbb{E}[I_b(X)]$.\n",
        "- Si $p_i = 1$ para algún $i$ (distribución degenerada), entonces $H_b(X) = 0$. Esto indica que no existe incertidumbre.\n",
        "**Cambio de base.** Si se desea relacionar entropías en diferentes bases, basta notar que\n",
        "$$\n",
        "\\log_b p_i = \\frac{\\log_c p_i}{\\log_c b}, \\qquad H_b(X) = \\frac{H_c(X)}{\\log_c b},\n",
        "$$\n",
        "lo cual implica que todas las entropías difieren únicamente por un factor constante. Por tanto, la elección de base fija la escala numérica pero no altera la estructura teórica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f0cf6b",
      "metadata": {},
      "source": [
        "### 4. Interpretaciones estadísticas y geométricas <a id=\"interpretaciones\"></a>\n",
        "\n",
        "La entropía de Shannon admite múltiples lecturas complementarias:\n",
        "\n",
        "1. **Promedio de longitud de código óptimo.** Mediante el teorema de codificación de Shannon, $H(X)$ es el límite inferior para la longitud media de cualquier código prefijo que represente los resultados de $X$ sin pérdidas.\n",
        "2. **Volumen geométrico.** En un espacio de secuencias largas $X^n$, la entropía determina la cardinalidad aproximada del conjunto típico: $|\\mathcal{T}_\\varepsilon^{(n)}| \\approx 2^{n H(X)}$.\n",
        "3. **Medida de dispersión.** Desde un enfoque estadístico, $H(X)$ cuantifica cuán extendida está la distribución: distribuciones más uniformes producen entropías mayores.\n",
        "4. **Expectativa de sorpresa.** La entropía es la esperanza de la auto-información, asentando su sustento probabilístico y su relación con la teoría de decisiones.\n",
        "\n",
        "Estas interpretaciones refuerzan la idea de que la entropía no mide el desorden absoluto, sino la incertidumbre promedio condicionada por la probabilidad de cada evento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f829b9f7",
      "metadata": {},
      "source": [
        "### 5. Propiedades fundamentales <a id=\"propiedades\"></a>\n",
        "\n",
        "> **Proposición 5.1 (No negatividad).** Para toda variable aleatoria discreta $X$ se cumple $H(X) \\geq 0$, con igualdad si y solo si $X$ es degenerada.\n",
        ">\n",
        "> **Demostración.** Como $\\forall i \\in \\{1, \\dots, n\\}$ se cumple $0 \\leq p_i \\leq 1$, se obtiene $\\log p_i \\leq 0$. En consecuencia, cada sumando verifica $-p_i \\log p_i \\geq 0$. Si $H(X) = 0$, entonces $\\forall i$ se verifica $p_i \\in \\{0, 1\\}$, ergo la distribución es determinista.\n",
        "\n",
        "> **Proposición 5.2 (Continuidad).** La función $p \\mapsto H(p)$ es continua en el simplex $\\Delta_{n-1}$. Además, es estrictamente cóncava.\n",
        ">\n",
        "> **Demostración.** La continuidad sigue del hecho de que $x \\log x$ es continua en $[0,1]$ y se extiende por continuidad en $x = 0$. La cóncavidad estricta proviene de la segunda derivada $\\frac{\\partial^2}{\\partial p_i^2} (-p_i \\log p_i) = -1/(p_i \\ln 2) < 0$ para $p_i > 0$, combinada con la restricción afín $\\sum_{i=1}^n p_i = 1$.\n",
        "\n",
        "> **Proposición 5.3 (Máximo para la distribución uniforme).** La entropía se maximiza en $\\Delta_{n-1}$ cuando $\\forall i$ se cumple $p_i = 1/n$, alcanzando el valor $H(X) = \\log n$.\n",
        ">\n",
        "> **Demostración.** Aplicando la desigualdad de Gibbs,\n",
        "> $$\n",
        "> H(X) = -\\sum_{i=1}^n p_i \\log p_i \\leq -\\sum_{i=1}^n p_i \\log \\frac{1}{n} = \\log n.\n",
        "> $$\n",
        "> La desigualdad es estricta salvo que $\\forall i$ se cumpla $p_i = 1/n$.\n",
        ">\n",
        "> **Proposición 5.4 (Aditividad para variables independientes).** Si $X$ e $Y$ son independientes, entonces $H(X, Y) = H(X) + H(Y)$.\n",
        ">\n",
        "> **Demostración.** Para $p_{ij} = \\mathbb{P}(X = x_i, Y = y_j) = p_i q_j$ con $\\forall i, j$, se obtiene\n",
        "> $$\n",
        "> H(X, Y) = -\\sum_{i, j} p_{ij} \\log p_{ij} = -\\sum_{i, j} p_i q_j (\\log p_i + \\log q_j) = H(X) + H(Y).\n",
        "> $$\n",
        "\n",
        "> **Proposición 5.5 (Descomposición condicional).** Para variables arbitrarias $X$ e $Y$, vale la identidad\n",
        "> $$\n",
        "> H(X, Y) = H(X) + H(Y \\mid X) = H(Y) + H(X \\mid Y).\n",
        "> $$\n",
        "> Esta propiedad, conocida como *regla de la cadena*, se deduce directamente de la definición de entropía condicional (sección [8](#extensiones)).\n",
        "\n",
        "Las proposiciones anteriores resumen las propiedades estructurales centrales. La cóncavidad proporciona, además, desigualdades útiles como la subaditividad y la invarianza frente a permutaciones del alfabeto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e116daf",
      "metadata": {},
      "source": [
        "### 6. Caracterización axiomática <a id=\"axiomas\"></a>\n",
        "\n",
        "Una de las justificaciones más poderosas de la fórmula de Shannon proviene de los **axiomas de Shannon-Khinchin**. Sean $F$ y $G$ funciones definidas en el simplex de probabilidad $\\Delta_{n-1}$. Se exige a una medida de incertidumbre $H$ cumplir:\n",
        "\n",
        "1. **Continuidad.** $H(p)$ varía continuamente con $p$.\n",
        "2. **Máximo para la uniforme.** $H$ alcanza su máximo en la distribución uniforme.\n",
        "3. **Expansibilidad.** Agregar un evento imposible no cambia la entropía: $H(p_1, \\dots, p_n, 0) = H(p_1, \\dots, p_n)$.\n",
        "4. **Recursividad (o aditividad condicional).** Para cualquier partición de eventos,\n",
        "$$\n",
        "H(p_1, \\dots, p_n) = H(p_1 + p_2, p_3, \\dots, p_n) + (p_1 + p_2) H\\left(\\frac{p_1}{p_1 + p_2}, \\frac{p_2}{p_1 + p_2}\\right).\n",
        "$$\n",
        "\n",
        "> **Teorema 6.1 (Shannon-Khinchin).** Toda función $H$ que satisface los axiomas anteriores está dada, salvo una constante multiplicativa positiva, por la entropía de Shannon.\n",
        "\n",
        "**Bosquejo de la demostración.**\n",
        "\n",
        "- La recursividad permite reducir la evaluación de $H$ en distribuciones arbitrarias a desplazamientos en particiones binarias.\n",
        "- Considerando repartos uniformes $(1/n, \\dots, 1/n)$ y usando inducción, se prueba que $H(1/n, \\dots, 1/n) = c \\log n$ para alguna constante $c > 0$.\n",
        "- La cóncavidad y la continuidad se emplean para extender la expresión logarítmica a distribuciones racionales y luego reales.\n",
        "- La expansibilidad garantiza consistencia cuando se agregan ceros, completando la caracterización.\n",
        "\n",
        "Este resultado legitima de manera unívoca la fórmula de Shannon como la única medida de incertidumbre compatible con los axiomas anteriores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11ad6d9",
      "metadata": {},
      "source": [
        "### 7. Ejemplos ilustrativos <a id=\"ejemplos\"></a>\n",
        "\n",
        "> **Ejemplo 7.1 (Variable binaria).** Sea $X$ con $\\mathbb{P}(X=1) = p$ y $\\mathbb{P}(X=0) = 1-p$. La entropía binaria es\n",
        "> $$\n",
        "> H(p) = - p \\log_2 p - (1-p) \\log_2 (1-p).\n",
        "> $$\n",
        "> El máximo $H(1/2) = 1$ ocurre cuando ambos resultados son equiprobables. Además, $H(p)$ es simétrica respecto de $p = 1/2$ y estrictamente cóncava.\n",
        "> **Ejemplo 7.2 (Dado sesgado).** Para un dado con probabilidades $p = (0.4, 0.2, 0.1, 0.1, 0.1, 0.1)$, la entropía es menor que la del dado justo ($\\log_2 6 \\approx 2.585$ bits), reflejando la asimetría.\n",
        "> **Ejemplo 7.3 (Fenómeno raro).** Si $p_1 = 0.99$ y el resto de probabilidades suman $0.01$, la entropía se aproxima a $0.081$ bits, indicando que el sistema es casi determinista.\n",
        "La comparación de estos escenarios evidencia cómo varía la incertidumbre en función de la distribución.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cbf837be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dado justo                     H = 2.5850 bits\n",
            "Dado sesgado                   H = 2.3219 bits\n",
            "Variable binaria (p=0.5)       H = 1.0000 bits\n",
            "Variable binaria (p=0.99)      H = 0.0808 bits\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def entropy(probabilities, base=2):\n",
        "    total = 0.0\n",
        "    for p in probabilities:\n",
        "        if p > 0:\n",
        "            total -= p * (math.log(p) / math.log(base))\n",
        "    return total\n",
        "\n",
        "examples = {\n",
        "    \"Dado justo\": [1/6] * 6,\n",
        "    \"Dado sesgado\": [0.4, 0.2, 0.1, 0.1, 0.1, 0.1],\n",
        "    \"Variable binaria (p=0.5)\": [0.5, 0.5],\n",
        "    \"Variable binaria (p=0.99)\": [0.99, 0.01]\n",
        "}\n",
        "\n",
        "for name, probs in examples.items():\n",
        "    h = entropy(probs)\n",
        "    print(f\"{name:30s} H = {h:.4f} bits\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb48946",
      "metadata": {},
      "source": [
        "El código anterior implementa directamente la definición de la entropía en base $2$ y confirma numéricamente los valores esperados de la teoría. Por ejemplo, una variable binaria equiprobable alcanza una entropía de un bit, mientras que un dado sesgado reduce su entropía respecto del dado ideal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2f3f18f",
      "metadata": {},
      "source": [
        "### 8. Entropía conjunta, condicional e información mutua <a id=\"extensiones\"></a>\n",
        "\n",
        "Sea $(X, Y)$ un par de variables aleatorias discretas con distribución conjunta $p_{ij}$.\n",
        "Denotamos los alfabetos por $\\mathcal{X} = \\{x_1, \\dots, x_n\\}$ y $\\mathcal{Y} = \\{y_1, \\dots, y_m\\}$, con probabilidades conjuntas $p_{ij} = \\mathbb{P}(X = x_i, Y = y_j)$.\n",
        "\n",
        "> **Definición 8.1 (Entropía conjunta).** $H(X, Y) = - \\sum_{i, j} p_{ij} \\log_2 p_{ij}.$\n",
        "\n",
        "> **Definición 8.2 (Entropía condicional).** $H(Y \\mid X) = - \\sum_{i, j} p_{ij} \\log_2 p_{j \\mid i}$, donde $p_{j \\mid i} = \\mathbb{P}(Y = y_j \\mid X = x_i)$.\n",
        "\n",
        "> **Definición 8.3 (Información mutua).** $I(X; Y) = H(X) + H(Y) - H(X, Y)$.\n",
        "\n",
        "Se derivan identidades clave:\n",
        "\n",
        "- **Regla de la cadena.** $H(X, Y) = H(X) + H(Y \\mid X)$.\n",
        "- **No negatividad de la información mutua.** $I(X; Y) \\geq 0$, con igualdad si y solo si $X$ e $Y$ son independientes.\n",
        "- **Condicional sobre condicional.** $H(Y \\mid X) \\leq H(Y)$, con igualdad de nuevo bajo independencia.\n",
        "\n",
        "**Demostración de la no negatividad.** A partir de la definición,\n",
        "$$\n",
        "I(X; Y) = \\sum_{i, j} p_{ij} \\log \\frac{p_{ij}}{p_i q_j} = D_{\\mathrm{KL}}(p_{ij} \\Vert p_i q_j),\n",
        "$$\n",
        "donde $D_{\\mathrm{KL}}$ es la divergencia de Kullback-Leibler, cantidad no negativa. La igualdad se alcanza únicamente cuando $p_{ij} = p_i q_j$, es decir, cuando $X$ e $Y$ son independientes.\n",
        "\n",
        "La información mutua mide cuán lejos está la distribución conjunta del producto de sus marginales, y por ende, cuánta información comparte el par de variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1e72b4",
      "metadata": {},
      "source": [
        "### 9. Desigualdades y límites <a id=\"desigualdades\"></a>\n",
        "\n",
        "1. **Desigualdad de Fano.** Relaciona la entropía condicional con la probabilidad de error en un problema de decisión:\n",
        "$$\n",
        "H(X \\mid Y) \\leq h_2(P_e) + P_e \\log(|\\mathcal{X}| - 1),\n",
        "$$\n",
        "   donde $h_2$ es la entropía binaria y $P_e$ la probabilidad de estimar $X$ erróneamente con base en $Y$.\n",
        "2. **Subaditividad.** Para cualquier par de variables, $H(X, Y) \\leq H(X) + H(Y)$. Esto se deduce de la cóncavidad de la entropía y de la identidad de la sección anterior.\n",
        "3. **Desigualdad de Jensen.** Usando la cóncavidad de $-\\log$, se obtiene la cota superior\n",
        "$$\n",
        "H(X) = - \\sum_{i=1}^n p_i \\log p_i \\leq - \\log \\left( \\sum_{i=1}^n p_i^2 \\right).\n",
        "$$\n",
        "   La igualdad se logra si y solo si $p$ es uniforme.\n",
        "4. **Cota inferior por la divergencia.** Para cualquier distribución de referencia $q$, se verifica\n",
        "$$\n",
        "H(X) = -\\sum_{i=1}^n p_i \\log p_i = -\\sum_{i=1}^n p_i \\log q_i + D_{\\mathrm{KL}}(p \\Vert q) \\leq -\\sum_{i=1}^n p_i \\log q_i.\n",
        "$$\n",
        "   La desigualdad se obtiene de la positividad de la divergencia de Kullback-Leibler.\n",
        "\n",
        "Estas desigualdades confieren herramientas cuantitativas para evaluar la incertidumbre en sistemas complejos y para establecer límites fundamentales en tareas de inferencia y comunicación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ef075d",
      "metadata": {},
      "source": [
        "### 10. Aplicaciones en codificación y ciencia de datos <a id=\"aplicaciones\"></a>\n",
        "\n",
        "- **Compresión sin pérdidas.** El teorema de fuente de Shannon establece que cualquier esquema de codificación prefijo tiene longitud promedio $L$ que satisface $H(X) \\leq L \\leq H(X) + 1$. Códigos de Huffman alcanzan esta cota de manera constructiva.\n",
        "- **Criptografía y seguridad.** La entropía cuantifica la imprevisibilidad de claves y contraseñas, determinando la resistencia frente a ataques por fuerza bruta.\n",
        "- **Aprendizaje automático.** Objetivos como la *cross-entropy* y el *log-loss* se basan en la entropía de Shannon y la divergencia de Kullback-Leibler para medir discrepancias entre distribuciones modelo y observadas.\n",
        "- **Termodinámica y física estadística.** En sistemas discretos de energía, la entropía de Shannon coincide con la entropía de Gibbs, estableciendo el puente entre información y mecánica estadística.\n",
        "\n",
        "Estas aplicaciones ilustran la ubicuidad de la entropía como herramienta cuantitativa transversal a la ingeniería, la ciencia y la matemática aplicada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11. Conclusiones y lecturas recomendadas <a id=\"conclusiones\"></a>\n",
        "\n",
        "En síntesis, la entropía de Shannon emerge como la única medida compatible con axiomas naturales de incertidumbre, exhibe propiedades robustas (no negatividad, cóncavidad, aditividad condicionada) y ofrece interpretaciones tanto estadísticas como operacionales. Su relevancia se extiende desde la teoría de códigos hasta la inferencia estadística y la física.\n",
        "\n",
        "**Lecturas recomendadas.**\n",
        "\n",
        "- C. E. Shannon, \"A Mathematical Theory of Communication\", *Bell System Technical Journal*, 27(3):379-423, 1948.\n",
        "- T. M. Cover y J. A. Thomas, *Elements of Information Theory*, segunda edición, Wiley-Interscience, 2006.\n",
        "- D. MacKay, *Information Theory, Inference, and Learning Algorithms*, Cambridge University Press, 2003.\n",
        "\n",
        "Estos textos profundizan en el formalismo, generalizaciones continuas y aplicaciones avanzadas de la entropía.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
