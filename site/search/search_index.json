{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Permutation Entropy for Volcano Forecasting","text":"<p>This site documents the permutation-entropy prototype for eruption forecasting. The toolkit combines information-theoretic complexity measures (PE, MPE, WPE) with basic machine-learning models to estimate eruption probabilities from continuous seismic data.</p> <ul> <li>Theory: Seismic context, Permutation entropy, MPE &amp; WPE, Time-series complexity, Application to eruption forecasting.</li> <li>Usage: Installation, Basic examples, CLI &amp; pipelines.</li> <li>API reference: Entropy, Features, Models &amp; pipelines.</li> <li>Paper scaffold: LaTeX outline for journal/thesis-style write-ups.</li> </ul>"},{"location":"api/entropy/","title":"Entropy API","text":"<p>Core functions live in <code>pevolc.entropy</code> and operate on 1D sequences (NumPy arrays or array-like).</p>"},{"location":"api/entropy/#compute_pesignal-order3-delay1-basee-normalizetrue","title":"<code>compute_pe(signal, order=3, delay=1, base=e, normalize=True)</code>","text":"<p>Permutation entropy of a 1D sequence. - <code>order</code>: embedding dimension \\(m\\) (number of samples per permutation). - <code>delay</code>: lag \\(\\tau\\) between embedded samples. - <code>base</code>: logarithm base for entropy (e, 2, or 10). - <code>normalize</code>: divide by \\(\\log_b m!\\) to constrain output to <code>[0, 1]</code>. Returns a single <code>float</code>.</p>"},{"location":"api/entropy/#compute_wpesignal-order3-delay1-basee-normalizetrue-weightsnone-weight_strategyvariance","title":"<code>compute_wpe(signal, order=3, delay=1, base=e, normalize=True, weights=None, weight_strategy=\"variance\")</code>","text":"<p>Weighted permutation entropy; identical interface to <code>compute_pe</code> with additional weighting. - <code>weights</code>: optional array of length <code>n_windows</code> to weight each ordinal pattern. - <code>weight_strategy</code>: if <code>weights</code> is not provided, choose <code>\"variance\"</code> or <code>\"energy\"</code> to compute per-window weights internally. Returns a single <code>float</code>.</p>"},{"location":"api/entropy/#compute_mpesignal-scales5-order3-delay1-basee-normalizetrue","title":"<code>compute_mpe(signal, scales=5, order=3, delay=1, base=e, normalize=True)</code>","text":"<p>Multiscale permutation entropy computed after coarse-graining. - <code>scales</code>: integer <code>k</code> to use scales <code>1..k</code> or an explicit iterable of scales. Returns a <code>list[float]</code> of length equal to the number of requested scales.</p>"},{"location":"api/entropy/#utilities","title":"Utilities","text":"<ul> <li><code>coarse_grain(signal, scale)</code> \u2013 average non-overlapping blocks of length <code>scale</code>; raises if the signal is too short.</li> <li><code>ordinal_pattern_indices(signal, order, delay)</code> \u2013 return dense indices (<code>0..m!-1</code>) for each embedded window using stable sorting to break ties.</li> <li><code>pattern_distribution(signal, order, delay, weights=None)</code> \u2013 return <code>(probs, counts)</code> arrays with optional weights applied; <code>probs</code> sums to 1 when patterns exist.</li> </ul>"},{"location":"api/features/","title":"Features API","text":"<p>Sliding-window feature extraction lives in <code>pevolc.features</code>.</p>"},{"location":"api/features/#windowconfig","title":"<code>WindowConfig</code>","text":"<p>Dataclass configuring entropy computation per window: - <code>window_seconds</code>, <code>step_seconds</code>: duration and hop size in seconds. - <code>order</code>, <code>delay</code>, <code>scales</code>: embedding parameters for PE/WPE/MPE. - <code>normalize</code>, <code>base</code>: entropy options. - <code>compute_pe</code>, <code>compute_mpe</code>, <code>compute_wpe</code>: toggles to enable/disable variants. - <code>detrend_signal</code>, <code>zscore</code>: optional preprocessing per trace.</p>"},{"location":"api/features/#extract_basic_featuressignal","title":"<code>extract_basic_features(signal)</code>","text":"<p>Return descriptive stats for a 1D sequence: mean, std, RMS, min, max.</p>"},{"location":"api/features/#extract_entropy_featuressignal-sampling_rate_hz-cfg","title":"<code>extract_entropy_features(signal, sampling_rate_hz, cfg)</code>","text":"<p>Compute PE/WPE/MPE (and basic stats) over sliding windows defined by <code>cfg</code>. - Inputs: 1D signal, sampling rate in Hz, and a <code>WindowConfig</code>. - Output: tidy <code>pandas.DataFrame</code> with <code>start_s</code>, <code>end_s</code>, entropy values (<code>pe</code>, <code>wpe</code>, <code>mpe_scale_k</code>), and <code>basic_*</code> amplitude stats.</p> <p>Internal helpers <code>_sliding_windows</code> and <code>_preprocess</code> are intentionally private; customise behaviour by extending <code>WindowConfig</code> or wrapping <code>extract_entropy_features</code>.</p>"},{"location":"api/models/","title":"Models, I/O, and Pipelines API","text":""},{"location":"api/models/#pevolcmodelspermutationentropyforecaster","title":"<code>pevolc.models.PermutationEntropyForecaster</code>","text":"<p>Wrapper around scikit-learn classifiers with optional Platt calibration. - <code>model_type</code>: <code>\"logreg\"</code>, <code>\"random_forest\"</code>, or <code>\"gradient_boosting\"</code>. - <code>fit(X, y)</code>: trains and, if <code>calibrate=True</code>, learns a calibration transform using a held-out split. - <code>predict_proba(X)</code>: calibrated eruption probabilities. - <code>predict_alert_level(X, thresholds=(0.33, 0.66))</code>: green/yellow/red based on probability thresholds. - Helpers: <code>train_forecasting_model(features, labels, model_type=\"logreg\")</code>, <code>predict_proba(model, features)</code>, and <code>evaluate_auc(model, features, labels)</code>.</p>"},{"location":"api/models/#calibration-utilities","title":"Calibration utilities","text":"<ul> <li><code>PlattCalibrator(max_iter=200)</code>: fit logistic calibration on raw probabilities (<code>fit</code>, then <code>transform</code>).</li> <li><code>calibrate_probabilities(probs, labels)</code>: one-liner to fit and apply <code>PlattCalibrator</code>.</li> </ul>"},{"location":"api/models/#io-helpers-pevolcio","title":"I/O helpers (<code>pevolc.io</code>)","text":"<ul> <li><code>read_waveform(path) -&gt; (data, sampling_rate)</code>: load miniSEED/SAC via ObsPy.</li> <li><code>load_waveforms(paths)</code>: convenience wrapper to read multiple files.</li> </ul>"},{"location":"api/models/#pipelines-pevolcpipelines","title":"Pipelines (<code>pevolc.pipelines</code>)","text":"<ul> <li><code>compute_entropy_dataset(data_paths, output_path, cfg)</code>: run sliding-window entropy extraction on multiple files, apply optional band-pass, infer labels, and save a CSV.</li> <li><code>run_from_config(config_path)</code>: load YAML and call <code>compute_entropy_dataset</code>.</li> <li><code>run_training(config_path)</code>: load dataset CSV, split into train/validation (time-aware or random), fit a <code>PermutationEntropyForecaster</code>, write metrics, calibration curves, and the model artifact.</li> </ul> <p>The command-line interface in <code>pevolc.cli</code> exposes <code>compute-entropy</code> and <code>train</code> commands that dispatch to these pipeline functions.</p>"},{"location":"paper/","title":"Paper Scaffold","text":"<p>This folder holds a LaTeX article that mirrors the Markdown theory pages. The intent is to keep heavy, paper-style typesetting isolated here while the canonical theory remains in Markdown under <code>docs/theory/</code>.</p> <ul> <li>Edit <code>main.tex</code> for journal/thesis-style output. Its structure tracks the theory pages, so copy text from Markdown and adapt notation as needed.</li> <li>Add references to <code>refs.bib</code>. A few seed entries are provided; extend with volcano-specific datasets or station reports.</li> <li>Build locally with <code>latexmk -pdf main.tex</code> (requires a LaTeX distribution with <code>latexmk</code>, <code>amsmath</code>, <code>graphicx</code>, <code>hyperref</code>).</li> </ul> <p>When the theory evolves, update Markdown first, then port key paragraphs or figures into this LaTeX scaffold to avoid divergence.</p>"},{"location":"theory/01-intro-seismicity/","title":"Introduction to Seismicity and Forecasting","text":"<p>Volcanic systems radiate continuous seismic energy as magma migrates, cracks open, and fluids interact with gas or groundwater. As eruptions approach, seismic waveforms often drift from low-energy background noise to tremor-like, quasi-harmonic, or bursty patterns. The goal of forecasting is to detect these transitions early enough to trigger mitigations while keeping false alarms manageable.</p>"},{"location":"theory/01-intro-seismicity/#eruption-precursors-in-seismic-data","title":"Eruption precursors in seismic data","text":"<ul> <li>Event-rate changes: volcano-tectonic or long-period earthquakes may cluster or accelerate.</li> <li>Emergent tremor: sustained, narrowband signals linked to fluid resonance or conduit oscillations.</li> <li>Amplitude growth: RMS energy rises as gas flux or magma ascent accelerates.</li> <li>Complexity swings: signals can become more regular (harmonic tremor) or more irregular (fracturing or turbulent degassing) depending on the underlying physics.</li> </ul>"},{"location":"theory/01-intro-seismicity/#forecasting-challenges","title":"Forecasting challenges","text":"<ul> <li>Non-stationarity from weather, seasonality, or cultural noise complicates baselines.</li> <li>Sensor diversity (station geometry, instrument response, site effects) changes waveform character.</li> <li>Data volume demands automated summarisation of long continuous streams.</li> <li>Label uncertainty arises because eruptions are rare, and onset times may be defined retroactively.</li> </ul> <p>Ordinal-pattern-based entropies offer a compact, interpretable descriptor of dynamical state that is invariant under monotonic transformations and tolerant of mild noise. They suit continuous monitoring where a handful of tuned parameters can track complexity shifts that accompany eruptive transitions.</p>"},{"location":"theory/02-permutation-entropy/","title":"Permutation Entropy","text":"<p>Permutation Entropy (PE) measures the diversity of ordinal patterns in a time series and is invariant to monotonic transformations. It compresses local temporal structure into a scalar complexity index.</p>"},{"location":"theory/02-permutation-entropy/#ordinal-patterns-and-embedding","title":"Ordinal patterns and embedding","text":"<p>For a series \\(x_t\\), choose an embedding dimension \\(m\\) and delay \\(\\tau\\). Form delayed vectors</p> \\[ \\mathbf{x}_t = \\big(x_t, x_{t+\\tau}, \\ldots, x_{t+(m-1)\\tau}\\big). \\] <p>Replace each \\(\\mathbf{x}_t\\) with the permutation \\(\\pi_t\\) that sorts its elements. A stable sort breaks ties by time order, so equal values retain their temporal ordering. There are \\(m!\\) possible patterns.</p>"},{"location":"theory/02-permutation-entropy/#probability-distribution-over-permutations","title":"Probability distribution over permutations","text":"<p>Let \\(n\\) be the number of windows. The empirical frequency of permutation \\(\\pi_i\\) is</p> \\[ p(\\pi_i) = \\frac{1}{n} \\sum_{t} \\mathbf{1}[\\pi_t = \\pi_i], \\] <p>optionally replaced by a weighted count in WPE (see below). The set \\(\\{p(\\pi_i)\\}\\) is a probability mass function over ordinal patterns.</p>"},{"location":"theory/02-permutation-entropy/#entropy-definition","title":"Entropy definition","text":"<p>The permutation entropy with logarithm base \\(b\\) is</p> \\[ H_{\\mathrm{PE}} = - \\sum_{i=1}^{m!} p(\\pi_i) \\log_b p(\\pi_i). \\] <p>Normalising by \\(\\log_b m!\\) yields \\(H_{\\mathrm{PE}} \\in [0, 1]\\).</p> <ul> <li>Low PE: highly regular signals (pure tone, clipped sensor, quasi-harmonic tremor).</li> <li>High PE: irregular or noisy signals (fracturing, turbulent degassing, cultural noise).</li> </ul>"},{"location":"theory/02-permutation-entropy/#practical-guidance-for-seismic-traces","title":"Practical guidance for seismic traces","text":"<ul> <li>Embedding dimension: \\(m \\in [3, 7]\\) is typical. Require at least \\(5\\)\u2013\\(10\\) times \\(m!\\) windows to populate permutations.</li> <li>Delay \\(\\tau\\): \\(\\tau=1\\) is broadband; larger \\(\\tau\\) emphasises lower-frequency ordering (useful when tremor dominates).</li> <li>Window length: choose windows long enough to gather permutations yet short enough to track transients (tens of seconds for many volcanoes).</li> <li>Ties and quantisation: strong quantisation or clipping lowers PE artificially; break ties by time as implemented here.</li> <li>Interpretation: a drop in PE with rising RMS often signals emergent, organised tremor; rising PE with rising RMS can reflect chaotic cracking or gas slug break-up.</li> </ul>"},{"location":"theory/03-mpe-wpe-variants/","title":"Multiscale and Weighted Permutation Entropy","text":"<p>Permutation entropy can be extended to capture structure across scales (MPE) and to emphasise energetic windows (WPE). Both reuse the ordinal-pattern machinery from plain PE.</p>"},{"location":"theory/03-mpe-wpe-variants/#multiscale-permutation-entropy-mpe","title":"Multiscale Permutation Entropy (MPE)","text":"<p>For a chosen scale \\(s\\), coarse-grain the signal by averaging non-overlapping blocks:</p> \\[ y^{(s)}_k = \\frac{1}{s} \\sum_{i=0}^{s-1} x_{ks+i}. \\] <p>Compute \\(H_{\\mathrm{PE}}\\) on \\(y^{(s)}\\) for each \\(s\\) in a scale set \\(\\mathcal{S}\\). The resulting curve \\(H_{\\mathrm{PE}}(s)\\) reveals how ordering changes when fast fluctuations are smoothed.</p> <ul> <li>Rising entropy with increasing \\(s\\) indicates additional irregularity at slower trends.</li> <li>Falling entropy suggests a dominant low-frequency order (e.g., tremor that becomes clearer once high-frequency noise is removed).</li> <li>Choose scales (e.g., \\(s=1\\ldots5\\)) such that each coarse-grained series still has enough windows to populate permutations.</li> </ul>"},{"location":"theory/03-mpe-wpe-variants/#weighted-permutation-entropy-wpe","title":"Weighted Permutation Entropy (WPE)","text":"<p>WPE modifies the empirical distribution with per-window weights \\(w_t\\):</p> \\[ p_w(\\pi_i) = \\frac{\\sum_t w_t \\,\\mathbf{1}[\\pi_t = \\pi_i]}{\\sum_t w_t}, \\qquad H_{\\mathrm{WPE}} = - \\sum_{i=1}^{m!} p_w(\\pi_i) \\log_b p_w(\\pi_i). \\] <p>Common choices are:</p> <ul> <li>Variance weighting: \\(w_t = \\operatorname{Var}(x_t, \\ldots, x_{t+(m-1)\\tau})\\).</li> <li>Energy weighting: \\(w_t = \\sum_j x_j^2\\) within the embedded window.</li> </ul> <p>Weights reduce the influence of low-amplitude background and amplify windows where the signal is energetic or rapidly varying.</p>"},{"location":"theory/03-mpe-wpe-variants/#relevance-to-volcanic-seismicity","title":"Relevance to volcanic seismicity","text":"<ul> <li>Emergent tremor is often energetic and quasi-periodic: WPE highlights these intervals while downweighting quiet background.</li> <li>Precursory changes can span seconds to minutes: MPE tracks how entropy evolves when short-period transients are smoothed.</li> <li>Using PE, MPE, and WPE together provides complementary descriptors that separate ordered tremor from bursty cracking and assess whether complexity sits at fast or slow scales.</li> </ul>"},{"location":"theory/04-time-series-and-complexity/","title":"Time-Series Complexity and Interpretation","text":"<p>Permutation-based entropies sit between spectral measures and nonlinear dynamical metrics. They capture ordinal structure without needing full phase-space reconstruction, making them robust to monotonic transformations and modest noise.</p>"},{"location":"theory/04-time-series-and-complexity/#relationship-to-other-measures","title":"Relationship to other measures","text":"<ul> <li>Amplitude-only entropy: Shannon entropy of amplitudes ignores ordering, so two signals with identical histograms but different dynamics look the same.</li> <li>Spectral metrics: periodograms or spectral slopes capture frequency content but lose phase/order information that distinguishes tremor from noise.</li> <li>Lyapunov exponents: probe divergence in reconstructed phase space but require long, clean trajectories and are fragile under noise or non-stationarity. PE focuses solely on the ordering of samples, providing a low-parameter view of dynamical structure.</li> </ul>"},{"location":"theory/04-time-series-and-complexity/#practical-interpretation","title":"Practical interpretation","text":"<ul> <li>High entropy: resembles white noise or highly irregular cracking/turbulence; pattern counts are nearly uniform.</li> <li>Low entropy: regular oscillations (harmonic tremor), clipped instruments, or quantised telemetry; a few permutations dominate.</li> <li>Temporal trends: a drop in entropy preceding an energy rise can indicate the system settling into a stable oscillatory regime; the opposite suggests chaotic activity or changing source processes.</li> </ul>"},{"location":"theory/04-time-series-and-complexity/#caveats","title":"Caveats","text":"<ul> <li>Ensure enough windows: very short windows bias toward low entropy because many permutations never occur.</li> <li>Ties and quantisation: rank ties are broken by time order; strong quantisation or clipping depresses entropy.</li> <li>Context matters: combine entropy with amplitude/energy statistics and station metadata to avoid misinterpreting cultural noise as volcanic change.</li> </ul> <p>In eruption forecasting, entropy acts as a low-cost monitor of dynamical state that complements rate- and energy-based indicators and integrates naturally into sliding-window pipelines.</p>"},{"location":"theory/05-application-to-eruption-forecasting/","title":"Applying Entropy Methods to Eruption Forecasting","text":"<p>Entropy features summarise waveform complexity and can signal dynamical shifts. The toolkit provides a lightweight pipeline aligned with the following conceptual workflow.</p> <ol> <li>Stream &amp; preprocess: detrend, band-pass, resample, and optionally z-score continuous traces; drop gross gaps.</li> <li>Window &amp; compute: run PE/WPE/MPE on overlapping windows (e.g., 10\u201360 s) with embedding \\((m, \\tau)\\) chosen per station.</li> <li>Aggregate features: combine entropy and basic amplitude statistics across recent windows to describe short-term evolution.</li> <li>Forecast: train a probabilistic classifier to estimate eruption likelihood over a horizon (e.g., 10\u201360 minutes ahead); calibrate probabilities.</li> </ol>"},{"location":"theory/05-application-to-eruption-forecasting/#physical-interpretations","title":"Physical interpretations","text":"<ul> <li>Rising entropy with rising energy may indicate fracturing or turbulent degassing.</li> <li>Falling entropy with rising energy often corresponds to emergent, quasi-harmonic tremor.</li> <li>Divergence across scales (MPE) highlights simultaneous tremor (ordered at low frequencies) and transient bursts (irregular at high frequencies).</li> </ul>"},{"location":"theory/05-application-to-eruption-forecasting/#alerting-and-calibration","title":"Alerting and calibration","text":"<ul> <li>Map calibrated probabilities to alert levels (green/yellow/red) with adjustable thresholds reflecting local risk tolerance.</li> <li>Use Platt scaling (implemented) or isotonic regression to align probabilities with observed frequencies and avoid overconfident alarms.</li> </ul>"},{"location":"theory/05-application-to-eruption-forecasting/#validation","title":"Validation","text":"<ul> <li>Prefer time-based splits (train early, validate late) to respect causality; random splits are acceptable only for quick checks.</li> <li>Report ROC/PR curves, reliability diagrams, alert lead time (how early an alert fires before eruption), and false-alarm rate per day/week.</li> <li>Track station-specific performance; strong site effects often require per-station tuning of \\((m, \\tau)\\) and MPE scales.</li> </ul>"},{"location":"theory/05-application-to-eruption-forecasting/#limitations-and-caveats","title":"Limitations and caveats","text":"<ul> <li>Eruptions are rare and labels uncertain. Bootstrapping, transfer learning between stations, and ensembling with rate/energy features can help.</li> <li>Entropy should complement, not replace, amplitude- or rate-based indicators and multi-parameter monitoring (deformation, gas, thermal).</li> </ul>"},{"location":"usage/01-installation/","title":"Installation","text":"<p>The toolkit targets Python 3.11+ and depends on NumPy/SciPy, pandas, scikit-learn, and ObsPy (for SAC/miniSEED reading).</p>"},{"location":"usage/01-installation/#clone-and-install","title":"Clone and install","text":"<p><pre><code>git clone https://github.com/AlmondSund/permutation-entropy.git\ncd permutation-entropy\npip install -e .\n</code></pre> If you prefer pinned versions, use: <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"usage/01-installation/#optional-extras","title":"Optional extras","text":"<ul> <li>Docs: <code>pip install -e .[docs]</code> installs MkDocs + Material and the PDF plugin to build the HTML/PDF documentation.</li> <li>Dev/testing: <code>pip install -e .[dev]</code> adds formatting, linting, and pytest.</li> </ul>"},{"location":"usage/01-installation/#environments-and-data","title":"Environments and data","text":"<ul> <li>A conda environment is described in <code>environment.yml</code> if you prefer conda/mamba.</li> <li>GPU/CUDA are not required; computations are CPU-only.</li> <li>Example configs live in <code>configs/</code>; example raw data can be downloaded via <code>scripts/download_example_data.py</code> (or swap in your own miniSEED/SAC files).</li> </ul>"},{"location":"usage/01-installation/#build-the-docs","title":"Build the docs","text":"<p><pre><code>pip install -e .[docs]\nmkdocs serve        # live preview at http://127.0.0.1:8000\nmkdocs build        # HTML output in site/\n</code></pre> To render a MathJax-aware PDF, use the separate PDF config and a Chromium installation: <pre><code># needs mkdocs-pdf-export-plugin and a local Chromium/Chrome\nmkdocs build -f mkdocs-pdf.yml\n</code></pre> This writes a combined PDF at <code>site/pdf/permutation-entropy-docs.pdf</code>. Set <code>chromium_executable</code> in <code>mkdocs-pdf.yml</code> if Chromium/Chrome lives elsewhere.</p> <p>Keep Markdown under <code>docs/theory/</code> as the canonical source; copy relevant passages into <code>docs/paper/main.tex</code> only when producing a paper-grade PDF.</p>"},{"location":"usage/02-basic-examples/","title":"Basic Examples","text":"<p>These snippets illustrate the core API. You can run them in a Python session or Jupyter notebook once the package is installed.</p>"},{"location":"usage/02-basic-examples/#compute-pe-on-a-single-trace","title":"Compute PE on a single trace","text":"<pre><code>import numpy as np\nfrom pevolc.entropy import compute_pe, compute_mpe, compute_wpe\n\nt = np.linspace(0, 60, 6000)\ntrace = np.sin(2 * np.pi * 1.2 * t)  # quasi-harmonic synthetic tremor\npe = compute_pe(trace, order=4, delay=2, base=2)\nmpe = compute_mpe(trace, scales=5, order=4, delay=2, base=2)\nwpe = compute_wpe(trace, order=4, delay=2, weight_strategy=\"variance\")\nprint(f\"PE={pe:.3f}, WPE={wpe:.3f}, MPE@scale3={mpe[2]:.3f}\")\n</code></pre>"},{"location":"usage/02-basic-examples/#sliding-window-features-for-a-seismic-trace","title":"Sliding-window features for a seismic trace","text":"<pre><code>from pathlib import Path\nfrom pevolc.features import WindowConfig, extract_entropy_features\nfrom pevolc.io import read_waveform\n\ndata, sr = read_waveform(Path(\"data/raw/example.mseed\"))\ncfg = WindowConfig(\n    window_seconds=10,\n    step_seconds=5,\n    order=4,\n    scales=4,\n    detrend_signal=True,\n    zscore=True,\n)\ndf = extract_entropy_features(data, sr, cfg)\nprint(df.filter(regex=\"^(start_s|pe|wpe|mpe)\").head())\n</code></pre>"},{"location":"usage/02-basic-examples/#train-a-simple-forecaster","title":"Train a simple forecaster","text":"<pre><code>from pevolc.models import PermutationEntropyForecaster\n\n# Suppose df contains columns: pe, wpe, mpe_scale_1..k, and a binary label column.\nfeature_cols = [c for c in df.columns if c.startswith(\"mpe_scale\") or c in {\"pe\", \"wpe\"}]\nX = df[feature_cols]\ny = df[\"label\"].astype(int)\n\nforecaster = PermutationEntropyForecaster(model_type=\"logreg\", calibrate=True).fit(X, y)\nprobs = forecaster.predict_proba(X.tail(5))\nalerts = forecaster.predict_alert_level(X.tail(5))\n</code></pre>"},{"location":"usage/02-basic-examples/#generate-a-tiny-synthetic-dataset","title":"Generate a tiny synthetic dataset","text":"<p>If you do not have local seismic data, you can still exercise the pipeline: <pre><code>python scripts/download_example_data.py --output-dir data/raw\npython scripts/compute_entropy_dataset.py configs/example_compute.yaml\npython scripts/train_model.py configs/example_train.yaml\n</code></pre> This produces <code>data/processed/entropy_features.csv</code> and a calibrated model in <code>experiments/last_run/</code>.</p>"},{"location":"usage/03-cli-and-pipelines/","title":"CLI and Pipelines","text":"<p>The CLI wraps the reproducible pipelines for entropy computation and forecasting. All configs are YAML so you can version control experiment settings.</p>"},{"location":"usage/03-cli-and-pipelines/#compute-an-entropy-feature-set","title":"Compute an entropy feature set","text":"<p><pre><code>python scripts/compute_entropy_dataset.py configs/example_compute.yaml\n# or via the package entry point\npython -m pevolc.cli compute-entropy configs/example_compute.yaml\n</code></pre> Key config fields (see <code>configs/example_compute.yaml</code>): <pre><code>data_glob: \"data/raw/*.mseed\"     # paths to raw waveforms (miniSEED/SAC or plain text/npy)\nsampling_rate_hz: 100             # used when data are generic text/npy\nwindow_seconds: 10\nstep_seconds: 5\norder: 4\ndelay: 1\nscales: 3\nbandpass_low: 1.0                 # optional Butterworth band-pass (Hz)\nbandpass_high: 15.0\ndetrend: true\nzscore: true\noutput_path: \"data/processed/entropy_features.csv\"\nlabel_mapping:\n  eruption: 1\n  background: 0\n  default: 0\nlabel_shift_windows: 1            # shift labels earlier to simulate forecast horizon\n</code></pre> Outputs: a tidy CSV with <code>start_s</code>, <code>end_s</code>, <code>pe</code>, <code>wpe</code>, <code>mpe_scale_k</code>, basic stats, and optional labels inferred from filenames or <code>label_value</code>.</p>"},{"location":"usage/03-cli-and-pipelines/#train-a-forecaster","title":"Train a forecaster","text":"<p><pre><code>python scripts/train_model.py configs/example_train.yaml\n# or\npython -m pevolc.cli train configs/example_train.yaml\n</code></pre> Example training config: <pre><code>dataset_path: \"data/processed/entropy_features.csv\"\nlabel_column: \"label\"\nfeature_columns: null   # auto-select numeric columns except label\nmodel_type: \"logreg\"    # or \"random_forest\", \"gradient_boosting\"\nsplit:\n  type: \"time\"          # \"time\" respects chronology; \"random\" for quick checks\n  val_fraction: 0.2\n  time_column: \"start_s\"\nmodel_path: \"experiments/last_run/forecaster.joblib\"\nmetrics_path: \"experiments/last_run/metrics.csv\"\nreliability_bins: 10\n</code></pre> The pipeline trains, calibrates probabilities with Platt scaling, writes metrics and reliability curves to <code>experiments/last_run/</code>.</p>"},{"location":"usage/03-cli-and-pipelines/#evaluate-a-saved-model","title":"Evaluate a saved model","text":"<p><pre><code>python scripts/evaluate_model.py experiments/last_run/forecaster.joblib data/processed/entropy_features.csv --label-column label\n</code></pre> This prints ROC/PR scores and can be adapted for continuous monitoring hooks.</p> <p>These pipelines are intentionally lightweight; duplicate a config in <code>configs/</code> to tune per station, change embedding parameters, or adjust alert thresholds.</p>"}]}